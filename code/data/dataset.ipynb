{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from mne.io import read_raw_eeglab\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import nibabel as nib\n",
    "from mne.io import read_raw_eeglab\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read JSON with custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainStimuliDataset(Dataset):\n",
    "    def __init__(self, json_path, frame_size=224):\n",
    "        self.json_path = json_path\n",
    "        self.frame_size = frame_size\n",
    "        self.data_dict = self.load_json_data(json_path)\n",
    "        self.calculate_length()\n",
    "        # union of all the available channels in EEG experiments\n",
    "        self.eeg_channels_ordered = [\n",
    "            'AF3', 'AF4', 'AF7', 'AF8', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', \n",
    "            'CP1', 'CP2', 'CP3', 'CP4', 'CP5', 'CP6', 'CPz', 'Cz', 'F1', 'F2',\n",
    "            'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'FC1', 'FC2', 'FC3', 'FC4',\n",
    "            'FC5', 'FC6', 'FT7', 'FT8', 'Fp1', 'Fp2', 'Fpz', 'Fz', 'O1', 'O2',\n",
    "            'Oz', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'PO3', 'PO4',\n",
    "            'PO7', 'PO8', 'POz', 'Pz', 'T7', 'T8', 'TP10', 'TP7', 'TP8', 'TP9'\n",
    "        ]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, int):\n",
    "            index = [index]\n",
    "        elif isinstance(index, slice):\n",
    "            index = list(range(index.start or 0, index.stop or len(self), index.step or 1))\n",
    "        else:\n",
    "            raise TypeError('{cls} indices must be integers or slices, not {idx}'.format(\n",
    "                cls=type(self).__name__,\n",
    "                idx=type(index).__name__,\n",
    "            ))\n",
    "        data_list = [self.get_data_from_index(idx) for idx in index]\n",
    "        # initialize lists\n",
    "        fmri_list = []\n",
    "        eeg_list = []\n",
    "        frames_list = []\n",
    "        for data in data_list:\n",
    "            # fmri\n",
    "            nii_img = nib.load(data['nifti_path'])\n",
    "            fmri_data = nii_img.get_fdata()\n",
    "            fmri = fmri_data[:, :, :, data['time_indices']['fmri']['idx']]\n",
    "            fmri_list.append(torch.from_numpy(fmri).to(dtype=torch.float))\n",
    "            # eeg\n",
    "            raw_data = read_raw_eeglab(data['eeglab_path'])\n",
    "            eeg_data = self.insert_zero_rows_in_array(raw_data)\n",
    "            eeg = eeg_data[:, data['time_indices']['eeg']['start_idx']:data['time_indices']['eeg']['end_idx']+1]\n",
    "            eeg_list.append(torch.from_numpy(eeg).to(dtype=torch.float))\n",
    "            # frames\n",
    "            frames_paths = [f\"frame_{frame_idx:04d}.pt\" for frame_idx in \n",
    "                           range(data['time_indices']['frames']['start_idx'], data['time_indices']['frames']['end_idx']+1)]\n",
    "            frames = list(map(lambda x: torch.load(os.path.join(data['frames_dir'], x)), frames_paths))\n",
    "            frames_list.append(torch.stack(frames))\n",
    "        # stack tensors\n",
    "        fmri_tensor = torch.stack(fmri_list)\n",
    "        eeg_tensor = torch.stack(eeg_list)\n",
    "        frames_tensor = torch.stack(frames_list)\n",
    "        # remove first dim for integer index \n",
    "        if len(index) == 1:\n",
    "            fmri_tensor = fmri_tensor.squeeze(0)\n",
    "            eeg_tensor = eeg_tensor.squeeze(0)\n",
    "            frames_tensor = frames_tensor.squeeze(0)\n",
    "        return {\n",
    "            'fmri': fmri_tensor,\n",
    "            'eeg': eeg_tensor,\n",
    "            'frames': frames_tensor\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "    \n",
    "    def load_json_data(self, json_path):\n",
    "        with open(json_path, \"r\") as file:\n",
    "            data_dict = json.load(file)\n",
    "        return data_dict\n",
    "    \n",
    "    def calculate_length(self):\n",
    "        count = 0\n",
    "        for key in self.data_dict.keys():\n",
    "            for sub in list(self.data_dict[key].keys())[1:]:\n",
    "                for ses in self.data_dict[key][sub].keys():\n",
    "                    for run in self.data_dict[key][sub][ses].keys():\n",
    "                        count += len(self.data_dict[key][sub][ses][run]['chunks'])\n",
    "        self.count = count\n",
    "        \n",
    "    def get_data_from_index(self, idx):\n",
    "        \"\"\"Should be enhanced for multiple indices, as it is called during the `self.__getitem__()`\"\"\"\n",
    "        current_index = idx    \n",
    "        for key in self.data_dict.keys():\n",
    "            for sub in list(self.data_dict[key].keys())[1:]:\n",
    "                for ses in self.data_dict[key][sub].keys():\n",
    "                    for run in self.data_dict[key][sub][ses].keys():\n",
    "                        count = len(self.data_dict[key][sub][ses][run]['chunks'])\n",
    "                        if current_index < count:\n",
    "                            time_indices = self.data_dict[key][sub][ses][run]['chunks'][str(current_index)]\n",
    "                            return {\n",
    "                                'frames_dir': self.data_dict[key]['frames_dir'],\n",
    "                                'nifti_path': self.data_dict[key][sub][ses][run]['nifti_path'],\n",
    "                                'eeglab_path': self.data_dict[key][sub][ses][run]['eeglab_path'],\n",
    "                                'time_indices': time_indices\n",
    "                            }\n",
    "                        else:\n",
    "                            current_index -= count\n",
    "                            continue\n",
    "                        \n",
    "    def insert_zero_rows_in_array(self, raw_data):\n",
    "        \"\"\"Sort channels in `raw_data`, and then insert zero rows for channels that are not included in `raw_data.ch_names`\"\"\"\n",
    "        raw_data_ordered = raw_data.reorder_channels(sorted(list(set(self.eeg_channels_ordered) & set(raw_data.ch_names))))\n",
    "        current_channels_ordered = raw_data_ordered.ch_names\n",
    "        good_indices = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while i < len(current_channels_ordered) and j < len(self.eeg_channels_ordered):\n",
    "            if current_channels_ordered[i] == self.eeg_channels_ordered[j]:\n",
    "                good_indices.append(j)    \n",
    "                i += 1\n",
    "                j += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        raw_data_array = raw_data_ordered.get_data()\n",
    "        raw_data_array_with_inserted_zero_rows = np.zeros((len(self.eeg_channels_ordered), raw_data_array.shape[1]))\n",
    "        for i, idx in enumerate(good_indices):\n",
    "            raw_data_array_with_inserted_zero_rows[idx] = raw_data_array[i]\n",
    "        return raw_data_array_with_inserted_zero_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    # get values\n",
    "    fmri_list = [x['fmri'] for x in data]\n",
    "    eeg_list = [x['eeg'] for x in data]\n",
    "    frames_list = [x['frames'] for x in data]\n",
    "    # stack tensors\n",
    "    fmri_tensor = torch.stack(fmri_list)\n",
    "    eeg_tensor = torch.stack(eeg_list)\n",
    "    frames_tensor = torch.stack(frames_list)\n",
    "    return {\n",
    "        'fmri': fmri_tensor,\n",
    "        'eeg': eeg_tensor, \n",
    "        'frames': frames_tensor\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BrainStimuliDataset('dataset.json')\n",
    "train_dataset, val_dataset = random_split(dataset, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch['fmri'].shape, batch['eeg'].shape, batch['frames'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual_stimuli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
