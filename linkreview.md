# LinkReview

- Here we have collect info about all the works that may be useful for writing our paper
- We divide these works by topic in order to structure them
- Each of the contributors is responsible for their part of the work, as specified in the table

> [!NOTE]
> This review table will be updated, so it is not a final version

| Topic | Title | Year | Authors | Paper | Code | Summary |
| :--- | :--- | ---: | :--- | :--- | :--- | :--- |
| Datasets with simultaneous fMRI-EEG signals <br> [@kisnikser](https://github.com/kisnikser) | An open-access dataset of naturalistic viewing using simultaneous EEG-fMRI | 2023 | Qawi K. Telesford et al. | [scientific data](https://www.nature.com/articles/s41597-023-02458-8) | [GitHub](https://github.com/NathanKlineInstitute/NATVIEW_EEGFMRI) | Recordings of **simultaneous** fRMI-EEG from 22 individuals across various visual and naturalistic stimulus. Visual tasks: flickering checkerboard, visual paradigm Inscapes, several short video movies. |
|      | Multisubject, multimodal face processing | 2024 | Daniel G Wakeman et al. | [scientific data](https://www.nature.com/articles/sdata20151) | [OpenNeuro](https://openneuro.org/datasets/ds000117/versions/1.0.6) | Containts **simultaneous** fMRI-EEG data for 16 participants. Visual stimuli: perceptual task on pictures of familiar, unfamiliar and scrambled faces during two visits to the laboratory. |
|      | Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film | 2022 | Julia Berezutskaya et al. | [scientific data](https://www.nature.com/articles/s41597-022-01173-0) | [GitHub #1](https://github.com/UMCU-RIBS/ieeg-fmri-dataset-validation), [GitHub #2](https://github.com/UMCU-RIBS/ieeg-fmri-dataset-quickstart) | Contains fMRI-EEG data for 51 participants, but non-simultaneous. Short audiovisual film stimulus, around 6 minutes of Pippi Longstocking. |
|      | Simultaneous EEG and functional MRI data during rest and sleep from humans | 2023 | Yameng Gu et al. | [Data in Brief](https://www.sciencedirect.com/science/article/pii/S2352340923001774) | [Download](https://openneuro.org/datasets/ds003768/versions/1.0.11) | Investigate spontaneous brain activity across distinct brain states. Contains fMRI-EEG data from 33 participants during the resting state and sleep. No visual stimuli. |
|      | Simultaneous and independent electroencephalography and magnetic resonance imaging: A multimodal neuroimaging dataset | 2023 | Jonathan Gallego-Rudolf et al. | [Data in Brief](https://www.sciencedirect.com/science/article/pii/S2352340923007461) | [Download](https://data.mendeley.com/datasets/crhybxpdy6/2) | Contains EEG and MRI data from 20 males performing eyes-open and eyes-closed tasks, with EEG recorded both inside and outside the MRI scanner. Total approximately 28 sessions. Not suitable for us, it is for studying the effect of simultaneously taking sensor readings. No visual stimuli. |   
|      | Le Petit Prince Hong Kong (LPPHK): Naturalistic fMRI and EEG data from older Cantonese speakers | 2024 | Mohammad Momenian et al. | [bioRxiv](https://www.biorxiv.org/content/10.1101/2024.04.24.590842v1) | [OpenNeuro](https://openneuro.org/datasets/ds004718/versions/1.1.0) | Sessions of fMRI and EEG for 52 participants. Audio stimuli: The Little Prince in Cantonese for approximately 20 minutes. Non-simultaneous. | 
|      | Multimodal single-neuron, intracranial EEG, and fMRI brain responses during movie watching in human patients | 2024 | Umit Keles et al. | [scientific data](https://www.nature.com/articles/s41597-024-03029-1) | [GitHub](https://github.com/OpenNeuroDatasets/ds004798), [OpenNeuro](https://openneuro.org/datasets/ds004798/versions/1.0.5) | Contains fMRI-EEG data for 20 participants. Visual stimuli: 8-min long excerpt from the video "Bang! You're Dead", arecognition memory test for movie content. |
| Methods using fMRI <br> [@DorinDaniil](https://github.com/DorinDaniil) | Natural scene reconstruction from fMRI signals using generative latent diffusion | 2023 | Furkan Ozcelik et al. | [arXiv](https://arxiv.org/abs/2303.05334) | [GitHub](https://github.com/ozcelikfu/brain-diffuser) | Reconstruct a low-quality image, using the pre-trained VDVAE. Enhance the fidelity of the generated image with a pre-trained diffusion model (e.g., SD 1.5), using image and text embeddings as a context for generation process. Use sklearn ridge regression as an fMRI encoder |
|      | fMRI-based Decoding of Visual Information from Human Brain Activity: A Brief Review | 2021 | Shuo Huang et al. | [Springer Link](https://link.springer.com/article/10.1007/s11633-020-1263-y) | - | Analyze architectures. The diagrams show that at the time of 2019, generative architectures are actively used. |
|      | Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity | 2023 | Zijiao Chen et al. | [arXiv](https://arxiv.org/abs/2305.11675) | [GitHub](https://github.com/jqin4749/MindVideo), [Website](https://www.mind-video.com/) | Decoding video sequences using fMRI data. SC-MBM encoder for fMRI and spatio-temporal attention is used. A frozen CLIP is used to encode the video, and they make a caption video and encode it using CLIP too, and then contrastive learning is used. Stable diffusion freezes. |
|      | High-resolution image reconstruction with latent diffusion models from human brain activity | 2023 | Yu Takagi et al. | [arXiv](https://arxiv.org/abs/2306.11536) | [GitHub](https://github.com/yu-takagi/StableDiffusionReconstruction) | The Natural Scenes Dataset (NSD) was used. First, the authors built an independent model that determined the ROI. The reconstruction process includes three main steps: 1. Prediction of latent representation z of the original image from fMRI signals in the early visual cortex. 2. Adding noise to the latent z representation through a diffusion process. 3. Decoding latent textual representations of c from fMRI signals in the superior (ventral) visual cortex and using the noise latent representation zt and decoded c as input data for U-Net denoising to obtain the final reconstructed image. |
|      |  Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors  | 2024 | Scotti P. et al.  |   [arXiv](https://arxiv.org/abs/2305.18274)   |   [GitHub](https://medarc-ai.github.io/mindeye/)    | The high-level pipeline is the core of MindEye as it maps voxels to CLIP image space to be fed through pretrained image generation models. The MLP backbone for our high-level pipeline maps flattened voxels to an intermediate space of size 257 × 768, corresponding to the last hidden layer of CLIP ViT/L-14. The backbone consists of a linear layer followed by 4 residual blocks and a final linear projector. The embeddings from the backbone are fed to an MLP projector and a diffusion prior in parallel. The whole model is trained end-to-end with the prior getting an MSE loss and the projector getting a bidirectional CLIP loss. The projector outputs can be used for retrieval tasks and the diffusion prior outputs can be used by generative models to reconstruct images. |
| Methods using EEG <br> [@sem-k32](https://github.com/sem-k32) | Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion | 2024 | Dongyang Li et al. | [arXiv](https://arxiv.org/abs/2403.07721v5) | [GitHub](https://github.com/dongyangli-del/EEG_Image_decode)  | EEG encoder = Transformer -> CNN (for spatiotemp. dependencies) -> MLP; EEG context vector is used to reconstruct image CLIP-vector. The latter is used in diffusion model to gen images |
|      | NeuroGAN: image reconstruction from EEG signals via an attention-based GAN | 2022 | Rahul Mishra et al. | [Springer Link](https://link.springer.com/article/10.1007/s00521-022-08178-1) | - | CNN encoder for EEG incorporated into GAN's generator. $$ Loss = Loss_{\text{GAN}} + Loss_{\text{image classification}} + Loss_{\text{perceptial loss}} $$ |
|      | EEG2IMAGE: Image Reconstruction from EEG Brain Signals | 2023 | Prajwal Singh et al. | [arXiv](https://arxiv.org/abs/2302.10121) | [GitHub](https://github.com/prajwalsingh/EEG2Image) | Individual EEG feature extractor (LSTM, constastive learning) + conditioned GAN for image generation |
|      | Image Reconstruction from Electroencephalography Using Latent Diffusion | 2024 | Teng Fei et al. | [arXiv](https://arxiv.org/abs/2404.01250) | [GitHub](https://github.com/desa-lab/EEG-Image-Reconstruction) |   info-gypsy   |
|      | Image classification and reconstruction from low‑density EEG | 2024 | Sven Guenther et. al. | [Nature Reports](https://www.nature.com/articles/s41598-024-66228-1.pdf) |  |   compare diffrent EEG-encoders for classification/reconstruction (diffusion conditioning) tasks   |
|      | DCAE: A dual conditional autoencoder framework for the reconstruction from EEG into image | 2023 | Hong Zeng et. al. | [ELSEVIER](https://www.sciencedirect.com/science/article/pii/S1746809422008941/pdfft?casa_token=z0Oo8Ta4XzYAAAAA:dWKusVo-7Q2LnOswCxTnUmq9dO0mYcmEEsoKyRxARZ4AiWUPeyRHB3Knk7gnKn5Fn7v1BJvAdQ&md5=9c72d87f28b37ae7865377f44dbff021&pid=1-s2.0-S1746809422008941-main.pdf) |  |   CNN EEG encoder to approxiamate DenseNet image vector + **UNet** architecture to generate image  |
|      | Photorealistic reconstruction of visual texture from EEG signals | 2021 | Suguru Wakita et. al. | [Frontiers](https://www.frontiersin.org/articles/10.3389/fncom.2021.754587/pdf) |  | uses VAE model. Input is image + EEG, proccessed separetely and then fused; output is image + EEG, separetely processed |
| SOTA fMRI encoders <br> [@DorinDaniil](https://github.com/DorinDaniil) |     |     |     |     |     |     |
|      |  Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors  | 2024 | Scotti P. et al.  |   [arXiv](https://arxiv.org/abs/2305.18274)   |   [GitHub](https://medarc-ai.github.io/mindeye/)    | **Input Data**: The encoder takes flattened voxels representing fMRI activity as input. **Linear Layer**: The first layer is a linear layer that transforms the input data into an intermediate space of size 257 × 768, corresponding to the last hidden layer of the CLIP ViT/L-14 model. **Residual Blocks**: Following the linear layer are four residual blocks, which help the model capture complex dependencies in the data. **Final Linear Projector**: After the residual blocks, there is a final linear projector that transforms the outputs into the final embedding space. |
| SOTA EEG encoders <br> [@sem-k32](https://github.com/sem-k32) |     |     |     |     |     |     |
|      |   EEGNet: a compact convolutional neural network for EEG-based brain–computer interfaces   |   2018   |   Vernon J Lawhern et. al.   |   [IOPScience](https://iopscience.iop.org/article/10.1088/1741-2552/aace8c)   |   -  |   CNN-model with decoupled time and space convolutions   |
|      |   TSception:A Deep Learning Framework for Emotion Detection Using EEG   |   2020   |   Yi Ding et. al.   |   [IEEE](https://ieeexplore.ieee.org/document/9206750)   |      |   another CNN-model   |
| SOTA methods for image generation <br> [@2001092236](https://github.com/2001092236) |     |     |     |     |     |     |
|      |  Versatile Diffusion: Text, Images and Variations All in One Diffusion Model    |   2024   | Xingqian Xl et. al.     |   [arXiv](https://arxiv.org/pdf/2211.08332)   |  [GitHub](https://github.com/SHI-Labs/Versatile-Diffusion)    |    based on image and/or text generates image and text. Uses VAE (input->latent), context encoders (different input modalities into one embedding space) and Diffusion Model|
|      |    High-resolution image reconstruction with latent diffusion models from human brain activity	  |   2023 | Yu Takagi et al.   |  [arXiv](https://arxiv.org/abs/2306.11536)    |   [GitHub](https://github.com/yu-takagi/StableDiffusionReconstruction)   |  CLIP for encoding text, LDM for conditional generation. They train encoder (Image+Text->fMRI) and decoder (fMRI->image). Use freezed NNs. Models: LDM, CLIP
|      |   Efficient-VDVAE: Less is more   | 2022  | Louay Hazami et. al. | [arXiv](https://arxiv.org/abs/2203.13751)    |  [github](https://github.com/Rayhane-mamah/Efficient-VDVAE)    | (about VDVAE): uses hierarchical VAE (very deep), no latent space collapse.  Train weights available (see github) |
|     | VQGAN-CLIP |    2022 |   Katherine Crowson et. al.    |  [arXiv](https://arxiv.org/abs/2204.08583)     |   [GitHub](https://github.com/nerdyrodent/VQGAN-CLIP)   |   VQGAN generates images, CLIP assesses how well images aligned with text prompts   |
|     | VQGAN      |   2020  |  Patric Esser et. al.     | [arXiv](https://arxiv.org/abs/2012.09841) | [GitHub](https://github.com/CompVis/taming-transformers) | CNN for encoding into discrete codebook, transformer for generation of tokens, CNN for decoding from generated tokens. Allows conditional generation.|
|     | StyleGAN2 (Analyzing and Improving the Image Quality of StyleGAN) | 2020    | Tero Karras et. al.    |  [arXiv](https://arxiv.org/abs/1912.04958)    |  [GitHub](https://github.com/NVlabs/stylegan2)    |      |
| |DiffiT (Diffusion Vision Transformers)      |  2023     |   [arXiv](https://arxiv.org/abs/2312.02139)   | [GitHub](https://github.com/NVlabs/DiffiT)     |      | |

